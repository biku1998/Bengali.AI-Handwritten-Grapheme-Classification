{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bengali handwritten computer vision challange notebook\n",
    "\n",
    "> Resources i have used\n",
    "* Pytorch dataset documentation [Here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "* Pytorch pre-trained models [Here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n",
    "* A lot of google searches ðŸ˜…\n",
    "\n",
    "* **Task**\n",
    "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F1095143%2Fa9a48686e3f385d9456b59bf2035594c%2Fdesc.png?generation=1576531903599785&alt=media\" style =\"height : 500px\">\n",
    "\n",
    "> The task is multiple classification, given an image you have to predict it's grapheme_root, consonant_diacritic and vowel_diacritic\n",
    "* **Approach**\n",
    "> I have used ResNe34 pre-trained model from pytorch, removed the last Linear layer, and added 3 Fully connected layer to classify each category.\n",
    "\n",
    "> **Outline**\n",
    "* I have converted all the data into images before hand, i.e reading all the parquet files and building images from each row. \n",
    "* Create a Dataset class for the data, For this just follow the pytorch docs and you will good to go.\n",
    "* Create Model\n",
    "* Train the Model\n",
    "* Create Dataset class for testset\n",
    "* create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBsT0V5WzrFt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import albumentations\n",
    "from torchvision import transforms, utils,models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from PIL import Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1585027966819,
     "user": {
      "displayName": "SOURABH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzI8b3bKj6EzkhmqlaVoF-1-VKe5-NIoIoK9J8iQ=s64",
      "userId": "05670567564810889409"
     },
     "user_tz": -330
    },
    "id": "s2CZxw5ZEjux",
    "outputId": "5f597206-df46-4ac4-e5d1-3b2504729e29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download all the data and put everything in a file called data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1585027967128,
     "user": {
      "displayName": "SOURABH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzI8b3bKj6EzkhmqlaVoF-1-VKe5-NIoIoK9J8iQ=s64",
      "userId": "05670567564810889409"
     },
     "user_tz": -330
    },
    "id": "pxJ7jZYazrFy",
    "outputId": "6379c3bc-b6e9-403d-efe5-687e6f30e0f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'class_map.csv',\n",
       " 'class_map_corrected.csv',\n",
       " 'sample_submission.csv',\n",
       " 'test.csv',\n",
       " 'test_image_data_0.parquet',\n",
       " 'test_image_data_1.parquet',\n",
       " 'test_image_data_2.parquet',\n",
       " 'test_image_data_3.parquet',\n",
       " 'train.csv',\n",
       " 'train_image_data_0.parquet',\n",
       " 'train_image_data_1.parquet',\n",
       " 'train_image_data_2.parquet',\n",
       " 'train_image_data_3.parquet',\n",
       " 'train_multi_diacritics.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The below method will convert all the parquet files to images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNGUwIMczrF2"
   },
   "outputs": [],
   "source": [
    "# Data-transform class\n",
    "\n",
    "def transform_data(files_location,set_ = 'Train'):\n",
    "    \"\"\"\n",
    "    This function will load all the parquet files one by one and convert them to images\n",
    "    parameters :\n",
    "    files_location : location where all the files are present i.e .parquet and csv files\n",
    "    set_ : train or test set,\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(files_location+f'{set_}_*.parquet')\n",
    "\n",
    "    for f in files:\n",
    "        df = pd.read_parquet(f)\n",
    "        image_ids = df.image_id.values\n",
    "\n",
    "        df = df.drop('image_id',axis = 1)\n",
    "\n",
    "        image_array = df.values\n",
    "\n",
    "        if os.path.isdir(f'./{set_}_images') == False:\n",
    "            os.mkdir(f'./{set_}_images')\n",
    "\n",
    "        print(f'Transforming {f} from {set_} :\\n')\n",
    "        for i,img_id in tqdm(enumerate(image_ids),total = len(image_ids),unit=\" image\"):\n",
    "            img = image_array[i,:].reshape(137,236).astype(float)\n",
    "            img_RGB  = Image.fromarray(img).convert(\"RGB\")\n",
    "            img_RGB.save(f'./{set_}_images/{img_id}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02OzCTDAzrF5"
   },
   "outputs": [],
   "source": [
    "files_location = \"./data/\"\n",
    "transform_data(files_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1585027967879,
     "user": {
      "displayName": "SOURABH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzI8b3bKj6EzkhmqlaVoF-1-VKe5-NIoIoK9J8iQ=s64",
      "userId": "05670567564810889409"
     },
     "user_tz": -330
    },
    "id": "4tEpCKv-5Zxx",
    "outputId": "7f12dc9c-6195-4831-e299-c2478fee94d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200840"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how many traning images we have\n",
    "\n",
    "len(os.listdir('./Train_images/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1585027969176,
     "user": {
      "displayName": "SOURABH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzI8b3bKj6EzkhmqlaVoF-1-VKe5-NIoIoK9J8iQ=s64",
      "userId": "05670567564810889409"
     },
     "user_tz": -330
    },
    "id": "PjtvhfMB5zlQ",
    "outputId": "0c1aec2e-81ad-4aa6-839b-23a54c515f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (200840, 5)\n",
      "New Train shape :  (160672, 5)\n",
      "Validation shape :  (40168, 5)\n"
     ]
    }
   ],
   "source": [
    "# creating a validation set from the Train folder\n",
    "\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "\n",
    "print('Train shape : ',df_train.shape)\n",
    "\n",
    "# let's randomly sample 20 percent of data and place it under df_val, then we will move all the files from Train_images\n",
    "# to Val_images which entries are present in df_val\n",
    "\n",
    "df_val = df_train.sample((20 * df_train.shape[0])//100)\n",
    "\n",
    "df_new_train = df_train.drop(df_val.index)\n",
    "\n",
    "print('New Train shape : ',df_new_train.shape)\n",
    "print('Validation shape : ',df_val.shape)\n",
    "\n",
    "if os.path.isdir('./annotations') == False:\n",
    "    os.mkdir('./annotations')\n",
    "\n",
    "df_new_train.to_csv('./annotations/Train_annotation.csv',index = False)\n",
    "df_val.to_csv('./annotations/Val_annotation.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tranning Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRKugbW823D_"
   },
   "outputs": [],
   "source": [
    "## Don't load the images in the __init__ , load it inside __getitem__ , memory efficient \n",
    "\n",
    "class BengaliHandwrittenDataset(Dataset):\n",
    "    \"\"\" Bengali Handwritten dataset \"\"\"\n",
    "\n",
    "    def __init__(self,csv_file,root_dir,transform  = \"train\",image_height = None,image_width = None,mean = None,std = None):\n",
    "        \"\"\"\n",
    "        Args : \n",
    "        csv_file : annotation file\n",
    "        root_dir : image dir\n",
    "        transform : set on which aug will be applied    \n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "        if transform == \"train\":\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.CenterCrop(100,200),\n",
    "                albumentations.Resize(self.image_height,self.image_width,always_apply=True),\n",
    "                albumentations.ShiftScaleRotate(rotate_limit=2,p = 0.9),\n",
    "                albumentations.Normalize(mean,std,always_apply=True)\n",
    "            ])\n",
    "        \n",
    "        if transform == \"val\":\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(self.image_height,self.image_width,always_apply=True),\n",
    "                albumentations.Normalize(mean,std,always_apply=True)\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        image_name = os.path.join(self.root_dir,self.labels.iloc[index,0])\n",
    "        image = io.imread(image_name+'.jpg')\n",
    "        image = self.aug(image = image)['image']\n",
    "        image = np.transpose(image,(2,0,1))\n",
    "        label = self.labels.iloc[index,1:-1]\n",
    "        label = np.array([label]).astype(np.long)\n",
    "\n",
    "        return torch.tensor(image),torch.tensor(label,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UgTriTx3R9n"
   },
   "outputs": [],
   "source": [
    "train_set = BengaliHandwrittenDataset('./annotations/Train_annotation.csv','./Train_images',transform = 'train',\n",
    "                                            image_height=224,image_width=224,\n",
    "                                            mean = (0.485,0.456,0.406),\n",
    "                                            std = (0.229,0.224,0.225))\n",
    "val_set = BengaliHandwrittenDataset('./annotations/Val_annotation.csv','./Train_images',transform = 'val',\n",
    "                                            image_height=224,image_width=224,\n",
    "                                            mean = (0.485,0.456,0.406),\n",
    "                                            std = (0.229,0.224,0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1585027969400,
     "user": {
      "displayName": "SOURABH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzI8b3bKj6EzkhmqlaVoF-1-VKe5-NIoIoK9J8iQ=s64",
      "userId": "05670567564810889409"
     },
     "user_tz": -330
    },
    "id": "69LKjjl78WtG",
    "outputId": "ce8038cf-d402-44de-dd26-a925aabafec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_set size = 160672\n",
      "Val_set size = 40168\n"
     ]
    }
   ],
   "source": [
    "print(f'Train_set size = {len(train_set)}')\n",
    "print(f'Val_set size = {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xMqPcQm9X2O"
   },
   "outputs": [],
   "source": [
    "def imshow(image,label):\n",
    "    image = image.numpy()\n",
    "    image = np.transpose(image,(1,2,0))\n",
    "    plt.imshow(image)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's take a sample of batch-size 4 and plot on of the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1585027969701,
     "user": {
      "displayName": "SOURABH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzI8b3bKj6EzkhmqlaVoF-1-VKe5-NIoIoK9J8iQ=s64",
      "userId": "05670567564810889409"
     },
     "user_tz": -330
    },
    "id": "vpzMfou28YQq",
    "outputId": "469581a9-1d4c-4191-a14b-339d39f48298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample batch_sizes : \n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 1, 3])\n",
      "zeroth sample : \n",
      "tensor([[[2.2318, 2.2318, 2.2318,  ..., 2.2147, 2.2147, 2.2147],\n",
      "         [2.2147, 2.2147, 2.2147,  ..., 2.2147, 2.2147, 2.2147],\n",
      "         [2.2147, 2.2147, 2.2147,  ..., 2.2147, 2.2147, 2.2147],\n",
      "         ...,\n",
      "         [2.2147, 2.2318, 2.2489,  ..., 2.2318, 2.2318, 2.2318],\n",
      "         [2.2147, 2.2318, 2.2489,  ..., 2.2318, 2.2318, 2.2318],\n",
      "         [2.2147, 2.2318, 2.2489,  ..., 2.2318, 2.2318, 2.2318]],\n",
      "\n",
      "        [[2.4111, 2.4111, 2.4111,  ..., 2.3936, 2.3936, 2.3936],\n",
      "         [2.3936, 2.3936, 2.3936,  ..., 2.3936, 2.3936, 2.3936],\n",
      "         [2.3936, 2.3936, 2.3936,  ..., 2.3936, 2.3936, 2.3936],\n",
      "         ...,\n",
      "         [2.3936, 2.4111, 2.4286,  ..., 2.4111, 2.4111, 2.4111],\n",
      "         [2.3936, 2.4111, 2.4286,  ..., 2.4111, 2.4111, 2.4111],\n",
      "         [2.3936, 2.4111, 2.4286,  ..., 2.4111, 2.4111, 2.4111]],\n",
      "\n",
      "        [[2.6226, 2.6226, 2.6226,  ..., 2.6051, 2.6051, 2.6051],\n",
      "         [2.6051, 2.6051, 2.6051,  ..., 2.6051, 2.6051, 2.6051],\n",
      "         [2.6051, 2.6051, 2.6051,  ..., 2.6051, 2.6051, 2.6051],\n",
      "         ...,\n",
      "         [2.6051, 2.6226, 2.6400,  ..., 2.6226, 2.6226, 2.6226],\n",
      "         [2.6051, 2.6226, 2.6400,  ..., 2.6226, 2.6226, 2.6226],\n",
      "         [2.6051, 2.6226, 2.6400,  ..., 2.6226, 2.6226, 2.6226]]])\n",
      "tensor([[64,  1,  0]])\n",
      "----------------------------------\n",
      "zeroth  sample plot -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's test the dataset with dataloader\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size = BATCH_SIZE,shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_set,batch_size = BATCH_SIZE,shuffle=True)\n",
    "\n",
    "iter_ = iter(train_loader)\n",
    "images,labels = iter_.next()\n",
    "\n",
    "print('sample batch_sizes : ')\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "print('zeroth sample : ')\n",
    "print(images[0])\n",
    "print(labels[0])\n",
    "print('----------------------------------')\n",
    "# let's plot zeroth sample\n",
    "print(\"zeroth  sample plot -------\")\n",
    "imshow(images[0],labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hcaXHfB8zJV"
   },
   "outputs": [],
   "source": [
    "# Let's write Model class\n",
    "\n",
    "class Model_ResNet_34(nn.Module):\n",
    "\n",
    "    def __init__(self,use_pretrained = False):\n",
    "        super(Model_ResNet_34,self).__init__()\n",
    "\n",
    "        if use_pretrained:\n",
    "            self.base_model_arc  = models.resnet34(pretrained = True)\n",
    "        else:\n",
    "            self.base_model_arc = models.resnet34()\n",
    "\n",
    "        self.base_model_components = list(self.base_model_arc.children())[:-1] # take all the layers except the last one\n",
    "\n",
    "        self.base_model = nn.Sequential(*self.base_model_components)\n",
    "        \n",
    "        # all the CNN layers will calculate good image representation\n",
    "        \n",
    "        # now we will add 3 fully connected layer that will classify 3 categories.\n",
    "        \n",
    "        self.fc_1 = nn.Linear(512,168)\n",
    "        self.fc_2 = nn.Linear(512,11)\n",
    "        self.fc_3 = nn.Linear(512,7)\n",
    "\n",
    "    def forward(self,x):\n",
    "        bs ,_,_,_ = x.shape\n",
    "        x = self.base_model(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(bs,-1)\n",
    "        fc_1 = self.fc_1(x)\n",
    "        fc_2 = self.fc_2(x)\n",
    "        fc_3 = self.fc_3(x)\n",
    "\n",
    "        return fc_1,fc_2,fc_3    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check if the model output is same as we except"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1585027972079,
     "user": {
      "displayName": "SOURABH KUMAR",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgzI8b3bKj6EzkhmqlaVoF-1-VKe5-NIoIoK9J8iQ=s64",
      "userId": "05670567564810889409"
     },
     "user_tz": -330
    },
    "id": "LCQPH9VHAuw9",
    "outputId": "cda63b7e-5568-4c29-9a8f-d2413b42e4fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 168]) torch.Size([4, 11]) torch.Size([4, 7])\n"
     ]
    }
   ],
   "source": [
    "# let' see if the model is working or not\n",
    "m_reset_34 = Model_ResNet_34()\n",
    "op_1,op_2,op_3 = m_reset_34(images)\n",
    "print(op_1.shape,op_2.shape,op_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will define\n",
    "    - Loss fn\n",
    "    - train fn\n",
    "    - validation loss calc fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxLIrfHfDJdB"
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs,targets):\n",
    "    op_1,op_2,op_3 = outputs\n",
    "    t_1, t_2, t_3 = targets[ : ,:,0].argmax(1),targets[ : ,:,1].argmax(1),targets[ : ,:,2].argmax(1)\n",
    "\n",
    "    l_1 = nn.CrossEntropyLoss()(op_1,t_1)\n",
    "    l_2 = nn.CrossEntropyLoss()(op_2,t_2)\n",
    "    l_3 = nn.CrossEntropyLoss()(op_3,t_3)\n",
    "\n",
    "    return (l_1 + l_2 + l_3) / 3\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader, dataset, model, opt):\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "\n",
    "    for batch_index,data  in tqdm(enumerate(dataloader), total = len(dataset) // dataloader.batch_size):\n",
    "        inputs,labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs,labels)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        del inputs, labels, outputs # to reduce memory usage in the GPU.\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def eval(dataloader, dataset, model,DEVICE):\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    counter = 0\n",
    "    for batch_index,data  in tqdm(enumerate(dataloader), total = len(dataset) // dataloader.batch_size):\n",
    "        inputs,labels = data\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs,labels)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        final_loss += loss.item() # if we don't do .item() here, you will run out of GPU memory as all the loss will be \n",
    "                                  # accumulated on the gpu, .item() will transfer them to cpu i.e store it on RAM\n",
    "        del inputs, labels, outputs # to reduce memory usage in the GPU.\n",
    "        torch.cuda.empty_cache()\n",
    "    return final_loss / counter   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itH3-6JQR8jG"
   },
   "outputs": [],
   "source": [
    "def learn(BATCH_SIZE,epochs,train_set,val_set):\n",
    "    train_loader = DataLoader(train_set,batch_size = BATCH_SIZE,shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(val_set,batch_size = 4,shuffle=True)\n",
    "\n",
    "    m_reset_34 = Model_ResNet_34(use_pretrained=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(m_reset_34.parameters(),lr = 1e-4)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        train(train_loader,train_set,m_reset_34,optimizer)\n",
    "        \n",
    "        if os.path.isdir('./Models') == False:\n",
    "            os.mkdir('./Models')\n",
    "            \n",
    "        torch.save(m_reset_34.state_dict(),f'./Models/m_reset_34_epoch_{e}.pth')\n",
    "        \n",
    "        val_score = eval(val_loader,val_set,m_reset_34,device)\n",
    "\n",
    "        print(f'Validation loss : {val_score}')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1030it [24:45,  1.47s/it]                                                                                                            \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10042/10042 [04:44<00:00, 35.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 0.0007841504124501263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1030it [25:32,  1.44s/it]                                                                                                            \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10042/10042 [04:49<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss : 0.0002276020365027859\n"
     ]
    }
   ],
   "source": [
    "learn(156,2,train_set,val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unpack test-data\n",
    "* Write dataset class for test-data\n",
    "* Write a fn that will create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming ./data\\test_image_data_0.parquet from Test :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 600.01 image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming ./data\\test_image_data_1.parquet from Test :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 600.13 image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming ./data\\test_image_data_2.parquet from Test :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 600.04 image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming ./data\\test_image_data_3.parquet from Test :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 749.96 image/s]\n"
     ]
    }
   ],
   "source": [
    "# let's unpack test sets\n",
    "files_location = \"./data/\"\n",
    "transform_data(files_location,set_ = \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset_class for Test sets\n",
    "\n",
    "class BengaliHandwrittenDataset_Test(Dataset):\n",
    "    \"\"\" Bengali Handwritten dataset \"\"\"\n",
    "\n",
    "    def __init__(self,csv_file,root_dir,transform  = \"val\",image_height = None,image_width = None,mean = None,std = None):\n",
    "        \"\"\"\n",
    "        Args : \n",
    "        csv_file : annotation file\n",
    "        root_dir : image dir\n",
    "        transform : set on which aug will be applied    \n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.img_id = self.labels.image_id.values\n",
    "        \n",
    "        if transform == \"val\":\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(self.image_height,self.image_width,always_apply=True),\n",
    "                albumentations.Normalize(mean,std,always_apply=True)\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        image_id = self.img_id[index]\n",
    "        image_name = os.path.join(self.root_dir,self.labels.iloc[index,1])\n",
    "        image = io.imread(image_name+'.jpg')\n",
    "        image = self.aug(image = image)['image']\n",
    "        image = np.transpose(image,(2,0,1))\n",
    "\n",
    "        return {\n",
    "            'image': torch.tensor(image),\n",
    "            'image_id':image_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(model,dataloader,device):\n",
    "    predictions = []\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for batch_index,data  in enumerate(dataloader):\n",
    "        inputs,img_ids = data['image'],data['image_id']\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        op_1,op_2,op_3 = model(inputs)\n",
    "        \n",
    "        _,op_1 = torch.max(op_1.data,1)\n",
    "        _,op_2 = torch.max(op_2.data,1)\n",
    "        _,op_3 = torch.max(op_3.data,1)\n",
    "        \n",
    "    \n",
    "        for ii,im_id in enumerate(img_ids):\n",
    "            predictions.append((f\"{im_id}_grapheme_root\",op_1[ii].item()))\n",
    "            predictions.append((f\"{im_id}_vowel_diacritic\",op_2[ii].item()))\n",
    "            predictions.append((f\"{im_id}_consonant_diacritic\",op_3[ii].item()))\n",
    "            \n",
    "    return predictions\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = BengaliHandwrittenDataset_Test('./data/test.csv','./Test_images/',\n",
    "                                            image_height=224,image_width=224,\n",
    "                                            mean = (0.485,0.456,0.406),\n",
    "                                            std = (0.229,0.224,0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testset,batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model_ResNet_34()\n",
    "MODEL_NAME = \"m_reset_34_epoch_1.pth\"\n",
    "model.load_state_dict(torch.load(f\"./Models/{MODEL_NAME}\")) # load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_submission = create_submission_file(model,test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame(data = list_submission,columns = ['row_id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_0_consonant_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_0_grapheme_root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_0_vowel_diacritic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       row_id  target\n",
       "0        Test_0_grapheme_root       0\n",
       "1      Test_0_vowel_diacritic       0\n",
       "2  Test_0_consonant_diacritic       0\n",
       "3        Test_0_grapheme_root       0\n",
       "4      Test_0_vowel_diacritic       0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Kaggle_Bengali_handwritten_challenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pytorch_env': conda)",
   "language": "python",
   "name": "python37564bitpytorchenvconda1a599ba2df0f415ab16aa82c434e76bb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
